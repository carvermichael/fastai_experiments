{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a15003-6520-452d-a0c5-37dd98d73b7c",
   "metadata": {},
   "source": [
    "# MNIST Model From \"Scratch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564eac3-adee-4d81-8b7e-8acec7c41562",
   "metadata": {},
   "source": [
    "Goal: To reproduce the second half of Chapter 4 from the Fast.ai book with as little referencing the material as possible. Looking up things in the documentation is allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599b751-db9b-4e64-aaab-eab1d0694eb3",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09bfb57-5fb5-475c-93de-1e31ae8ddcdf",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- get data into tensors\n",
    "- initialize params (weights + bias(es))\n",
    "- multiply params by pixels (feed forward)\n",
    "- measure loss\n",
    "- get gradients\n",
    "- step with gradients and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39da374e-7638-4923-b9d9-7e15635d5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.external import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7e5cf6-1504-42f8-8999-20f5f5016bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88649b31-5f4b-47e5-8f80-5e8cb432d6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/carver/.fastai/data/mnist_sample/train/3/47123.png\n",
      "/home/carver/.fastai/data/mnist_sample/train/7/7420.png\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path.BASE_PATH = data\n",
    "\n",
    "train3s = data/'train'/'3'\n",
    "print(train3s.ls()[0])\n",
    "\n",
    "train7s = data/'train'/'7'\n",
    "print(train7s.ls()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b279be3b-4e1c-421e-ac2f-0ef3960f1780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABDElEQVR4nGNgoD9ghNKXtfe+Znh6hOHDQSyKqq79/fv339+/v49PtMfQycDBqRT8WNyegdv4X1s9LntYLf/+no/TFQl//37BIcWR8PXvlwTscqpr//594YZViqf+yd+/d5SwSTGmPPv79+/fpazYJN3+/vv79+/fvxdFsUhq/Ps/k43BZ/W/y2JYjOXgYGRgYGAq/3sBKsKCkPz/g4GBgYHh328GFagIE6YJBlh9AgF63/4eQ+Iq8SPL3fj7yxuJ//XeTHWo59hrvv39tQXuRAYGmWvcDAznzl1kYJCy4ddjeF0/E9kWlzkf/v79+/fv339/f3yZqYKQgES2jpE7A4PDzxOnNt7H41S6AABltWTLWj/2mgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAtUlEQVR4nGNgGDaAEc5yCff8s6TqxlEGBgaGIwvRlK3/hwC3IUJMcMm1vxAK+aTQjGWQ95ViYNC4wZ/J+P+qLg5XTPz377gaDjmOO/8+FOOQY1ry718CLn01//61MuGQjPj377MIDjnL3/8+e+OQEz7x7w8uOYYl//4txiXn/+ffOQ4ccpxn/j3xwaVxxb/fmbjkzL7/68clx30Jtw8Zmv79i8clp/bh3zRmXJIaHx/y45LDDgCG6EiZ+ZE2DwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(train3s.ls()[0])\n",
    "image.show()\n",
    "image7 = Image.open(train7s.ls()[0])\n",
    "image7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a98d91-9546-4e2a-9bdb-2aa605fa0001",
   "metadata": {},
   "source": [
    "### Array_Interfaces and Fastbook's Tensor Function\n",
    "\n",
    "This bit of code below (more or less copied from the [04_mnist_basics notebook](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)) took a while to comprehend. It turns out that the `array` method is a numpy construct and the `tensor` method is a fastbook construct.\n",
    "\n",
    "The `array` method uses the [numpy __array_interface__ protocol](https://numpy.org/doc/stable/reference/arrays.interface.html) and [the `tensor` method](https://docs.fast.ai/torch_core.html#tensor) uses the same thing to turn this image into a 2D array, but then calls [torch.as_tensor](https://pytorch.org/docs/stable/generated/torch.as_tensor.html) to create a PyTorch Tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5f35571-4aa9-4ba2-8525-6e588962ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 211 254 187]\n",
      " [  0   0   0 122 214 253]\n",
      " [  0   0   0   0   8  17]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "tensor([[  0,   0,   0, 211, 254, 187],\n",
      "        [  0,   0,   0, 122, 214, 253],\n",
      "        [  0,   0,   0,   0,   8,  17],\n",
      "        [  0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "print(array(image)[4:10, 4:10])\n",
    "\n",
    "from fastbook import tensor\n",
    "print(tensor(image)[4:10, 4:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d358e86-2caf-4f31-9c5f-0b84a990e379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tens = tensor(image)\n",
    "sample_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59285dfd-cba2-4ee1-bd91-f4da13e46d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Get all the training images into individual tensors, then stack them.\n",
    "\n",
    "all_valid_3s = [tensor(Image.open(path)) for path in train3s.ls()]\n",
    "stacked_valid_3s = torch.stack(all_valid_3s).float()/255\n",
    "print(stacked_valid_3s.shape)\n",
    "\n",
    "all_valid_7s = [tensor(Image.open(path)) for path in train7s.ls()]\n",
    "stacked_valid_7s = torch.stack(all_valid_7s).float()/255\n",
    "print(stacked_valid_7s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96631c2d-28f6-43fd-b7e8-e04cb7b23247",
   "metadata": {},
   "source": [
    "### Now we want to create a list of tuples (x, y) such that x is an array of pixels and y is 0 or 1 (where 1 signifies that the image is a 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df594195-0849-4049-996a-94361ed90448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 784])\n",
      "torch.Size([6265, 784])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_3s_count = stacked_valid_3s.shape[0]\n",
    "stacked_valid_3s = torch.reshape(stacked_valid_3s, (valid_3s_count,-1))\n",
    "print(stacked_valid_3s.shape)\n",
    "\n",
    "valid_7s_count = stacked_valid_7s.shape[0]\n",
    "stacked_valid_7s = torch.reshape(stacked_valid_7s, (valid_7s_count,-1))\n",
    "print(stacked_valid_7s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8728d62d-91c9-47a5-8e80-4c1e99550d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]),\n",
       " tensor([0.0000, 0.0000, 0.0196, 0.2235, 0.9922, 0.9843, 0.6235, 0.0000, 0.0000, 0.0000]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat([stacked_valid_3s, stacked_valid_7s])\n",
    "x.shape, x[0, 210:220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77ffc833-9e9f-447b-bb16-f8faf1c40dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Couple ways to do this bit...\n",
    "# y = torch.cat([tensor([1 for i in range(valid_3s_count)]), tensor([0 for i in range(valid_7s_count)])])\n",
    "y = torch.cat([tensor([1] * valid_3s_count), tensor([0] * valid_7s_count)]).unsqueeze(1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8f91ab2c-5268-4dd1-8f18-23e84454f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y = list(zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5451fa97-60b2-44c9-b1ae-ac4088746e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "tensor([ 1.0893,  0.3820, -2.3425,  0.2997], grad_fn=<SliceBackward0>)\n",
      "tensor([-1.7377], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create tensor with initialized weights and bias, set up for getting gradients.\n",
    "from numpy.random import normal\n",
    "\n",
    "weights = tensor(normal(size=784)).requires_grad_()\n",
    "print(weights.shape)\n",
    "print(weights[0:4])\n",
    "bias = tensor(normal(size=1)).requires_grad_()\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee82ecd-bbf7-466e-b2a8-298cbf044bd1",
   "metadata": {},
   "source": [
    "Now I'm lost...\n",
    "\n",
    "At this point, I have all the x and y inputs in tensors and the weights and bias in tensors (params).\n",
    "\n",
    "I can't remember how things were done from here on out though...\n",
    "\n",
    "I _think_ that there was an example done with just one image, doing the loss, gradient, and step. Then something was done with the matrix multiply operator in Python (`@`). I guess we just did the same steps there, on the CPU, before moving to the GPU..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9f72f-e399-4f3e-9bdc-e4d8fc472953",
   "metadata": {},
   "source": [
    "### Okay, I'm lost. Looking at the Reference Now.\n",
    "\n",
    "Here's what I missed:\n",
    "- Forgot to normalize the image arrays.\n",
    "- Need to list the zip (a list of a zip is a common python idiom).\n",
    "- Forgot to unsqueeze the training y tensor (this probably would have been obvious if I did the list of zip and inspected).\n",
    "- Used a reshape instead of a view (a reshape may or may not return a new tensor, whereas a view does not move the underlying data) -- not sure this is a mistake, per se.\n",
    "- I used the param version of requires_grad, which is fine for the way I initialized the params. But init_params in the chapter multiplies by a std number before calling the requires_grad_() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc1ac8-ee87-4922-b566-ca6386d2cd44",
   "metadata": {},
   "source": [
    "### Now I've read through the reference again. Trying to continue it without referencing further..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8f5d8ae3-9298-4551-afd6-fc76375c59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(x, weights, bias): return x@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6380202a-1c38-44cf-9494-691da68cc94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8000, 0.5000, 0.8000])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let me see if I can do the torch.where thing...\n",
    "preds = tensor([0.2, 0.5, 0.8])\n",
    "targets = tensor([1, 1, 0])\n",
    "torch.where(targets==1, 1-preds, preds)\n",
    "\n",
    "# Had to look a bit (of course), but got it pretty close the first time. Main thing was that I forgot to make preds and targets tensors (just had them as arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1c0e95fc-8ea7-432c-a3e2-dd1da03ddcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's try to get a mini-batch going...\n",
    "\n",
    "from fastai.data.load import DataLoader\n",
    "\n",
    "dl = DataLoader(dataset=x_y, batch_size=4, shuffle=True)\n",
    "batch_x, batch_y = dl.one_batch()\n",
    "batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f85e4bc5-832f-4328-8726-db4cbf04866a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "411ca0c5-bb10-47b4-9d73-ee8b9bdbf0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.9452, 11.2940, -5.3258,  9.3484], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = linear1(batch_x, weights, bias)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8b4b8259-c43a-4454-9771-6c5ead1a9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to look up this function.\n",
    "def sigmoid(x): return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "770ec5e1-c362-49aa-947b-59b20e887886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9996, 1.0000, 0.0048, 0.9999], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sigmoid(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1fdda82b-a4f2-4c20-9696-76363ab3ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(targets, predictions): return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aa4e6b76-e916-44c9-9dc3-437f14ae0eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3745, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(batch_y, predictions)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "720fa0f9-985f-4375-9ff9-905b68bc9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0af939a6-5312-48e7-be69-5356bc9305f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1139/18095485.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642969563/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  weights.grad, bias.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad, bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6bcf0-abd1-420a-a815-37d03642e871",
   "metadata": {},
   "source": [
    "### I guess the requires_grad_() error I had made _was_ important...\n",
    "\n",
    "Going back and trying again, leaving the above error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cb8160da-a311-467c-99e3-5f9953bf28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7add8fcb-fd69-4ad9-a500-0b3256c127bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor(-7.0804e-05), tensor([-0.0007]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad.shape, weights.grad.mean(), bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5f5b9-0b5c-4bff-a2f7-f971e460db97",
   "metadata": {},
   "source": [
    "### Okay, I got a grad, but the shapes are different from the example.\n",
    "\n",
    "The book's `weights.grad.shape` was `[784,1]`, but I'm just getting `[784]`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef93783-1646-4afb-bce6-11a0fb5cc5fe",
   "metadata": {},
   "source": [
    "I'm super tired at this point, so I'm just going to finish this with referencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "75ea5071-5fc5-44e4-8934-820fe002ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb, weights, bias)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ad8a7049-0634-4fca-9dd3-de00a224bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if there's a difference between these two...\n",
    "weights.grad = None\n",
    "bias.grad = None\n",
    "\n",
    "# weights.grad.zero_()\n",
    "# bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0df02123-b52d-41f5-8664-1a54c336057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, weights, bias):\n",
    "    for xb, yb in dl:\n",
    "        calc_grad(xb, yb, linear1)\n",
    "        weights.data -= weights.grad * lr\n",
    "        weights.grad.zero_()\n",
    "        bias.data -= bias.grad *lr\n",
    "        bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "06aaf7c6-ef51-4bc2-9379-1becad9b73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6fb6254a-fc42-43cf-92d2-9ecefcaa9c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6250)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(linear1(batch_x, weights, bias), batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedc738-b58f-41bd-b0bc-776426a88ae4",
   "metadata": {},
   "source": [
    "### Stopping Here. I'm totally exhausted. \n",
    "\n",
    "I'll get 'em next time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
