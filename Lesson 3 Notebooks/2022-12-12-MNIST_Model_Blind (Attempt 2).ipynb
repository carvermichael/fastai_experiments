{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6bd786-8af5-4506-b9d3-1864513893ae",
   "metadata": {},
   "source": [
    "# MNIST Model From \"Scratch\" -- Attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff56952d-f771-408a-80f6-af34f13ec701",
   "metadata": {},
   "source": [
    "Goal: To reproduce the second half of Chapter 4 from the Fast.ai book with as little referencing the material as possible. Looking up things in the documentation is allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce8b8dd-fa02-4172-ab5c-ddf83bc04a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.external import untar_data, URLs\n",
    "from pathlib import Path\n",
    "\n",
    "data = untar_data(URLs.MNIST_SAMPLE)\n",
    "Path.Base_Path = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eab27d9-62c8-4425-bb78-3a1630f6b03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+0lEQVR4nGNgGGSAkYGBQfzLVwYGBg55hu+PMCUfvXrLsF7CzYLhU1sXhu55/2DgdHEkE5qkwKF//+5dvnz58p1///6tlkSTFXHOFWRgYGDga/33799MbE5j5i5/8u7fv3+PsUkaQe09h02S5+K/f//+/dvhj02SwezLv38TMlmQRJBcfuo+A4ODAjtWjQwM0r6v//273IpDlsHkyb9/f5txyXKEr/vzohY9lBCg+N+/NkwbO3gYGBgYGATW/vvbBXEzI1xSYMr7t3onT4fLev5nYCiagK5XdBM8eiZh8UvR9Xv33v/79+/fATRjYUAz8FLWtgNXcDqYxgAAgnhyj4XP2k4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open((data/'valid'/'3').ls()[0])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8199b38e-6481-49bc-8269-cb69b0d9170f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28, 28]), torch.Size([28, 28]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# These imports are sloppy...\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "valid_3_tens = [torch.as_tensor(array(Image.open(im))) for im in (data/'valid'/'3').ls()]\n",
    "valid_7_tens = [torch.as_tensor(array(Image.open(im))) for im in (data/'valid'/'7').ls()]\n",
    "valid_3_tens[0].shape, valid_7_tens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd974e6-a20f-451d-85ce-08bfe474e034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens_stacked = torch.stack(valid_3_tens)\n",
    "valid_7_tens_stacked = torch.stack(valid_7_tens)\n",
    "valid_3_tens_stacked.shape, valid_7_tens_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27041c66-78cd-4abd-a952-5f13e1e1f38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 784]), torch.Size([1028, 784]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3s_view = valid_3_tens_stacked.view(-1, 28*28).to(torch.float64)/255\n",
    "valid_7s_view = valid_7_tens_stacked.view(-1, 28*28).to(torch.float64)/255\n",
    "valid_3s_view.shape, valid_7s_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e65e47-7ee4-42e1-91c3-937b010e9971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat([valid_3s_view, valid_7s_view])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a63202-330e-4d34-a6ce-1a80f284e08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2038, 1]),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.cat([torch.as_tensor([1] * len(valid_3s_view)), torch.as_tensor([0] * len(valid_7s_view))]).unsqueeze(-1)\n",
    "y.shape, y[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bfa1fea-f2a0-4247-bd1b-5bcce9655161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4544, -0.8108, -1.9538, -1.3584, -1.3897,  0.0122, -0.1243,  0.0540,\n",
       "        -2.3786, -0.2573,  0.1538, -0.2044,  1.2569, -0.0211,  2.1899, -0.3923,\n",
       "        -0.5945, -1.0011,  0.5230,  0.6760,  1.6610,  2.2277,  0.7094,  1.6383,\n",
       "         0.5006, -0.1470,  0.5744,  0.7620,  0.1112,  0.0907, -0.4662,  0.1981,\n",
       "        -0.2238, -0.0942,  1.0714,  0.2125,  0.2238, -1.2645, -0.3131,  0.6188,\n",
       "        -0.5867, -0.0473,  1.3559, -0.9897, -0.1669, -1.0384,  0.2056,  0.3999,\n",
       "        -1.5592, -1.7413,  0.2792,  0.2361,  1.1848,  0.4932, -0.2027, -0.7021,\n",
       "         0.7435, -1.0416, -0.2277,  1.0773,  0.4993, -1.3881, -0.3282,  0.6901,\n",
       "         1.1101,  0.2161, -0.7514, -0.2215, -1.1100, -0.1379, -0.7076, -1.2148,\n",
       "        -0.1597,  1.3133,  1.1880, -0.2805, -0.7836,  0.8343, -1.5228,  0.1492,\n",
       "        -1.1477,  0.4082,  0.1313,  0.6022, -1.5428,  0.7545, -0.2222, -0.3892,\n",
       "        -0.3660, -0.5062, -0.6368,  1.3330, -1.7704, -0.0309,  1.4297, -1.1189,\n",
       "         0.9362,  0.8946,  0.9181, -0.2820], dtype=torch.float64,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm not totally sure this bit is right. I think I want values that are between 0 and 1. We'll see...\n",
    "\n",
    "def init_param(shape, std=1.0): return (torch.as_tensor(np.random.randn(shape)) + std).requires_grad_()\n",
    "\n",
    "weights = torch.as_tensor(np.random.randn(784)).requires_grad_()\n",
    "bias = torch.as_tensor(np.random.randn(1)).requires_grad_()\n",
    "weights.shape, bias.shape\n",
    "weights[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf884048-086d-42cc-b084-8166a22be2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(x): return x@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ccfda95-322a-43c9-b9ab-71d42191baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(preds, targets): return torch.where(targets==1, 1-preds, preds).mean() # initially forgot the mean, didn't catch it at first because my initial sample was only one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7986259-c6bc-44db-9050-1e6f71cca85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4000, 0.1000, 0.1000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss test\n",
    "\n",
    "preds = torch.as_tensor([0.4, 0.1, 0.9])\n",
    "targets = torch.as_tensor([0, 0, 1])\n",
    "mnist_loss(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4074e0e9-ebe9-4e47-a339-5017c9e40895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.6534], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One round of SGD for a 1-image batch, \"manually\"\n",
    "\n",
    "pred_sample = linear1(x[0])\n",
    "pred_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1645c0fd-ff9b-4402-a732-4c6bfd19e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damn, I really messed this up somewhere...\n",
    "\n",
    "# Not sure what that 2.4... is supposed to signify...\n",
    "\n",
    "# Maybe I just to sigmoid that number, then compare it to the 0 or 1 to give the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62bae875-25d8-4d46-8886-6ba65b13137d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0658], dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sample.sigmoid_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bfdefc2-350c-4b39-8fb2-b951c1627396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9342], dtype=torch.float64, grad_fn=<WhereBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(pred_sample, y[0])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e6bfcd-8065-4c08-8d26-36b630a635db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "weights.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f7595d-da9d-4b61-962c-c6f2ae758439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([1]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad.shape, bias.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de9db654-671f-45db-bdf9-97f8326563e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_once(params, lr=0.001):\n",
    "    for param in params:\n",
    "        param.data -= param.grad * lr\n",
    "        param.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6afb216-d1a5-414e-b602-7d936d1e0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_once([weights, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "672da9d7-19c7-47b4-871b-5bc2ba200592",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e76ba971-48c6-43d6-bfd2-2c81e7a05b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't understand how the weights aren't leaf tensors...\n",
    "\n",
    "# Ohhhh, it has to do with the fact that I was pulling a single weight off before updating it. I need to update the entire tensor at once...\n",
    "\n",
    "# ...Yep! That did it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cef2fde9-6fe0-4d8f-ada2-a46d621d678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0662], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9338], dtype=torch.float64, grad_fn=<WhereBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred_sample_2 = linear1(x[0]).sigmoid()\n",
    "print(pred_sample_2)\n",
    "loss = mnist_loss(pred_sample_2, y[0])\n",
    "print(loss)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "075ea941-a337-43d7-acb8-ccc1e31be351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d889629e-8a7d-4c39-9627-50b110a96f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zip up Xs and Ys, get into dataloader, train for an epoch\n",
    "\n",
    "training_data = list(zip(x, y))\n",
    "training_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "897acab2-0924-472f-b3f6-61cd97ce7609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.data.load.DataLoader at 0x7f00c42425f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.data.load import DataLoader\n",
    "\n",
    "dl = DataLoader(dataset=training_data, batch_size=250)\n",
    "dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c52539-810d-4193-8cc4-375629bd1146",
   "metadata": {},
   "source": [
    "### Wow, I was about to train on the validation data...hahaha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8542090-3ece-4459-ac16-217da5429dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model):\n",
    "    for xb, yb in dataloader:\n",
    "        preds = model(xb).sigmoid() # probably should have put the sigmoid in the model\n",
    "        loss = mnist_loss(preds, yb)\n",
    "        print('loss: ', loss)\n",
    "        loss.backward()\n",
    "        step_once([weights, bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e306ac-91a6-47a0-9843-0d31fe153468",
   "metadata": {},
   "source": [
    "### Stopping here, because it's almost bed time.\n",
    "\n",
    "Recap:\n",
    "- Got through a round of gradient updates! Solid progress.\n",
    "- Didn't look at the reference notebook once!\n",
    "- Got the data into tensors really quickly.\n",
    "- Missed:\n",
    "    - Was about to train on the validation set.\n",
    "    - Tried to update the weight gradients one-by-one, instead of doing the whole tensor at once.\n",
    "    - Forgot to normalize the image data again, then ran into some weirdness with float32 vs float64. Had to explicitly cast, not sure why that happened this time.\n",
    "    - Forgot to take the mean of the losses (after the torch.where call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e0a67-cd2e-4ff6-84d5-fc9fb16a6756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
