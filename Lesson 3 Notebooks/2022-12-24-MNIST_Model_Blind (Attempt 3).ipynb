{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3c4fbc-acd8-4e51-86dc-509946bf9e04",
   "metadata": {},
   "source": [
    "# MNIST Model From \"Scratch\" -- Attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaed2cd-d013-48e5-b0a9-d1f1bca247e0",
   "metadata": {},
   "source": [
    "Goal: To reproduce the second half of Chapter 4 from the Fast.ai book with as little referencing the material as possible. Looking up things in the documentation is allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1956e8f4-196d-487d-acb0-622b0a214951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import untar_data, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ae565d-1bb6-49c6-aa60-b48266241786",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02f6d11-48d3-4c10-b2ee-049354780375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('labels.csv'),Path('valid'),Path('train')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path.BASE_PATH = data\n",
    "data.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feba1d25-060e-48e3-ac99-74888d282c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...],\n",
       " (#6265) [Path('train/7/10002.png'),Path('train/7/1001.png'),Path('train/7/10014.png'),Path('train/7/10019.png'),Path('train/7/10039.png'),Path('train/7/10046.png'),Path('train/7/10050.png'),Path('train/7/10063.png'),Path('train/7/10077.png'),Path('train/7/10086.png')...])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3s = (data/'train'/'3').ls().sorted()\n",
    "train7s = (data/'train'/'7').ls().sorted()\n",
    "train3s, train7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5383d03-7487-4d1d-802f-565fb299ba24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from numpy import *\n",
    "\n",
    "train3_tens = [torch.as_tensor(array(Image.open(im))) for im in train3s]\n",
    "train7_tens = [torch.as_tensor(array(Image.open(im))) for im in train7s]\n",
    "len(train3_tens), len(train7_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88710771-d2f7-415b-876c-ca7cdfb7822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3_stacked = (torch.stack(train3_tens).float()) / 255\n",
    "train7_stacked = (torch.stack(train7_tens).float()) / 255\n",
    "train3_stacked.shape, train7_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98feec79-f14b-43ce-9725-bf9f573e10e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1647, 0.4627, 0.8588, 0.6510])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3_stacked[0][4][10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853a57f3-b03b-4dff-9cc3-7b2b16cb8e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid3_stacked = (torch.stack([torch.as_tensor(array(Image.open(im))) for im in (data/'train'/'3').ls().sorted()]).float()) / 255\n",
    "valid7_stacked = (torch.stack([torch.as_tensor(array(Image.open(im))) for im in (data/'train'/'7').ls().sorted()]).float()) / 255\n",
    "valid3_stacked.shape, valid7_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c69e0bb3-2fae-4df9-8c4a-cd46552aeab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]),\n",
       " torch.Size([1]),\n",
       " tensor(0.0198, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Almost forgot again about the torch randn...\n",
    "\n",
    "weights = (torch.randn((28*28, 1))).requires_grad_() # I was adding 1.0 to all the weights, for some unknown reason...\n",
    "bias = (torch.randn((1)) + 1.0).requires_grad_()\n",
    "weights.shape, bias.shape, weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "46ac2e64-ae6d-4a4c-bf75-61eb185547b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4036], grad_fn=<SelectBackward0>),\n",
       " tensor(1.9356, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm not sure if I should be getting something between 0 and 1...not sure if that's right for weights.\n",
    "# And if it's not, why does that matter?\n",
    "\n",
    "weights[2], bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "829284d6-ca19-43ef-908c-9b58ef15109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265, torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = torch.cat([torch.as_tensor(([1] * len(valid3_stacked))),\n",
    "                torch.as_tensor(([0] * len(valid7_stacked)))]).unsqueeze(-1)\n",
    "len(valid3_stacked), len(valid7_stacked), ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "86048852-6a3f-4c3e-aa1a-57f07fb82b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12396, torch.Size([12396, 784]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = torch.cat([valid3_stacked, valid7_stacked]).view(-1, 28*28)\n",
    "xys = list(zip(xs, ys))\n",
    "len(xys), xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d66a441d-41a9-449a-a5a2-d79e1eb3166d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.data.load import DataLoader\n",
    "\n",
    "dset = DataLoader(xys, bs=256, shuffle=True)\n",
    "dset.one_batch()[0].shape, dset.one_batch()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e664c658-dc7d-4b00-8d0e-487d68837227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(xb): return xb@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a0dcb997-c6f5-40b5-924b-bb2a317757fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(preds, tars): \n",
    "    # print(f\"preds: {preds[0]}, tars: {tars[0]}\")\n",
    "    temp = torch.where(tars == 1, 1-preds, preds)\n",
    "    # print(f\"loss before mean: {temp[0]} -- shape: {temp.shape}\")\n",
    "    return temp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04488a4c-075a-4170-b310-fe28008c9c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: 0.10000000149011612, tars: 1\n",
      "loss before mean: 0.8999999761581421 -- shape: torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_test = mnist_loss(torch.as_tensor([0.1, 0.8, 0.4]), torch.as_tensor([1, 1, 0]))\n",
    "loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ee7bf02-cb4c-49c6-837d-bdfde263fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(preds, valid_ys): return (((preds > 0.5) == valid_ys)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b20e6b4-18f2-47b4-991d-b1438f79c163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3333)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = batch_accuracy(torch.as_tensor([.4, .9, .55]), torch.as_tensor([1, 0, 1]))\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9ea2a4c-d4d7-41e7-b47c-33a193260ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_xs = torch.cat([valid3_stacked, valid7_stacked]).view(-1, 28*28)\n",
    "valid_ys = torch.cat([torch.as_tensor([1] * len(valid3_stacked)), torch.as_tensor([0] * len(valid7_stacked))])\n",
    "valid_xs.shape, valid_ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "651eeff5-7588-4485-b697-cc8e6cac20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_batch_accuracy():\n",
    "    preds = linear1(valid_xs).sigmoid()\n",
    "    print(batch_accuracy(preds, valid_ys).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "91558b3b-a928-47cc-874f-987dd0d466c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(lr=0.01, pr=True):\n",
    "    for xb, yb in dset:\n",
    "        preds = linear1(xb)\n",
    "        # print(f\"preds: {preds[0]}\")\n",
    "        preds = preds.sigmoid_() # initially forgot the sigmoid here\n",
    "        # print(f\"preds: {preds[0]}\")\n",
    "        loss = mnist_loss(preds, yb)\n",
    "        # I was printing the loss here, but that doesn't really make sense. I'd want the loss and accuracy over the entire dataset, not from each batch...\n",
    "        # if pr: print(loss.item(), end=' ')\n",
    "        # print(f\"loss: {loss}\")\n",
    "        loss.backward()\n",
    "        print(f\"weights grad: {weights.grad.mean()}, bias grad: {bias.grad}\")\n",
    "        for param in (weights, bias):\n",
    "            # print(f\"before: {param.data[0]}, grad: {param.grad[0]}\")\n",
    "            param.data -= param.grad * lr\n",
    "            # print(f\"after: {param.data[0]}\")\n",
    "            param.grad = None\n",
    "    if pr: calc_batch_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1d543f5e-3618-4f10-bd44-5d45345e30ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights grad: -2.8590175134013407e-05, bias grad: tensor([-0.0002])\n",
      "weights grad: -0.00012591762060765177, bias grad: tensor([-0.0013])\n",
      "weights grad: -0.00013849942479282618, bias grad: tensor([-0.0011])\n",
      "weights grad: -3.768434180528857e-05, bias grad: tensor([-0.0004])\n",
      "weights grad: 7.102797098923475e-05, bias grad: tensor([0.0008])\n",
      "weights grad: -5.5793901992728934e-05, bias grad: tensor([-0.0005])\n",
      "weights grad: 3.5758985177380964e-05, bias grad: tensor([0.0004])\n",
      "weights grad: -6.720107921864837e-05, bias grad: tensor([-0.0005])\n",
      "weights grad: -4.967653876519762e-05, bias grad: tensor([-0.0003])\n",
      "weights grad: 8.608797361375764e-05, bias grad: tensor([0.0010])\n",
      "weights grad: 5.8634832384996116e-05, bias grad: tensor([0.0005])\n",
      "weights grad: 4.104158142581582e-05, bias grad: tensor([0.0005])\n",
      "weights grad: -6.640130595769733e-05, bias grad: tensor([-0.0004])\n",
      "weights grad: -0.00010795096022775397, bias grad: tensor([-0.0008])\n",
      "weights grad: -3.420831490075216e-05, bias grad: tensor([-0.0003])\n",
      "weights grad: 6.219735951162875e-05, bias grad: tensor([0.0005])\n",
      "weights grad: 1.1291211876596208e-06, bias grad: tensor([-0.0002])\n",
      "weights grad: -2.4589184249634854e-05, bias grad: tensor([-0.0003])\n",
      "weights grad: 3.595206635509385e-06, bias grad: tensor([-2.0406e-06])\n",
      "weights grad: 7.417934102704749e-05, bias grad: tensor([0.0006])\n",
      "weights grad: 6.541879702126607e-05, bias grad: tensor([0.0007])\n",
      "weights grad: -0.00011821933003375307, bias grad: tensor([-0.0010])\n",
      "weights grad: -9.056019189301878e-05, bias grad: tensor([-0.0009])\n",
      "weights grad: 1.7100923287216574e-05, bias grad: tensor([0.0002])\n",
      "weights grad: -1.497521316196071e-05, bias grad: tensor([-3.9389e-06])\n",
      "weights grad: 0.0001033913649735041, bias grad: tensor([0.0010])\n",
      "weights grad: 2.5346573693241226e-06, bias grad: tensor([0.0001])\n",
      "weights grad: -1.7919517631526105e-05, bias grad: tensor([-9.9602e-05])\n",
      "weights grad: 5.657642395817675e-05, bias grad: tensor([0.0005])\n",
      "weights grad: -3.414465390960686e-05, bias grad: tensor([-0.0002])\n",
      "weights grad: 0.0001914501772262156, bias grad: tensor([0.0016])\n",
      "weights grad: -7.245434244396165e-05, bias grad: tensor([-0.0007])\n",
      "weights grad: -3.089991514571011e-05, bias grad: tensor([-0.0003])\n",
      "weights grad: -8.20353852759581e-06, bias grad: tensor([-4.3779e-05])\n",
      "weights grad: 9.712170867715031e-05, bias grad: tensor([0.0009])\n",
      "weights grad: 0.00017986763850785792, bias grad: tensor([0.0015])\n",
      "weights grad: 0.0001350836391793564, bias grad: tensor([0.0008])\n",
      "weights grad: -2.7435311494627967e-05, bias grad: tensor([-0.0002])\n",
      "weights grad: 5.0515911425463855e-05, bias grad: tensor([0.0005])\n",
      "weights grad: -3.4309935017518e-06, bias grad: tensor([-7.8163e-05])\n",
      "weights grad: 1.9685572624439374e-05, bias grad: tensor([0.0002])\n",
      "weights grad: 3.214976823073812e-05, bias grad: tensor([0.0003])\n",
      "weights grad: -0.00011427314893808216, bias grad: tensor([-0.0009])\n",
      "weights grad: 4.53682332590688e-05, bias grad: tensor([0.0004])\n",
      "weights grad: -2.7849293473991565e-05, bias grad: tensor([-0.0003])\n",
      "weights grad: 4.073578384122811e-05, bias grad: tensor([0.0003])\n",
      "weights grad: -5.960543057881296e-05, bias grad: tensor([-0.0002])\n",
      "weights grad: -6.343412678688765e-05, bias grad: tensor([-0.0002])\n",
      "weights grad: -0.00017807383846957237, bias grad: tensor([-0.0015])\n",
      "0.5000837445259094\n"
     ]
    }
   ],
   "source": [
    "one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db918b7e-e335-4280-8be3-0d155eba57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_epochs(n):\n",
    "    for i in range(n):\n",
    "        one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7b970006-a164-449a-b760-f3043fce3901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5000837445259094\n",
      "0.5000837445259094\n",
      "0.5000837445259094\n",
      "0.5000837445259094\n",
      "0.5000837445259094\n",
      "0.5000837445259094\n",
      "0.5000837445259094\n",
      "0.5000837445259094\n",
      "0.5000837445259094\n",
      "0.5000837445259094\n"
     ]
    }
   ],
   "source": [
    "n_epochs(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5aaaa9-dcd9-438e-a527-5d1414a90f0d",
   "metadata": {},
   "source": [
    "Okay, I messed up something here...\n",
    "\n",
    "My gradients are all zero. What did I mess up?\n",
    "\n",
    "I'm stumped... Going to take a look at the last attempt's code...\n",
    "\n",
    "So the way I'm doing the loss doesn't seem to be right. The result isn't a tensor, and therefore the gradient is getting lost (or can't be calculated through).\n",
    "\n",
    "...\n",
    "\n",
    "The gradient is now being persisted (not sure how to phrase that) up until the where call, but only until the mean call. \n",
    "\n",
    "Hmm, now everything is moving, but not by much...\n",
    "Going to reinit the params, then try again. Might need to bump the lr?\n",
    "\n",
    "This is a mess, and my brain is fried. I'm calling it...ugh."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
